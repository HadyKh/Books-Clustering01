{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7534de59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hadyo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hadyo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hadyo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = DeprecationWarning)\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn import svm\n",
    "from sklearn import tree\n",
    "from urllib import request\n",
    "from random import randint\n",
    "#from wordcloud import WordCloud\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.models import LdaModel, Word2Vec\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#!pip install mlxtend\n",
    "#from mlxtend.evaluate import bias_variance_decomp\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection  import cross_val_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2111929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_url_list = [\"https://www.gutenberg.org/files/19719/19719-0.txt\",\n",
    "                  \"https://www.gutenberg.org/cache/epub/28434/pg28434.txt\",\n",
    "                  \"https://www.gutenberg.org/cache/epub/15147/pg15147.txt\",\n",
    "                  \"https://www.gutenberg.org/cache/epub/17866/pg17866.txt\",\n",
    "                  \"https://www.gutenberg.org/files/3772/3772-0.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccffe308",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fb2497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function read raw data\n",
    "def read_books(url):\n",
    "    files = []\n",
    "    for i in url:\n",
    "        file = requests.get(url = i)\n",
    "        files.append(file.content.decode('utf-8'))\n",
    "    return files\n",
    "\n",
    "#read raw data\n",
    "raw_dataset = read_books(books_url_list)\n",
    "len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92462147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Function cleaned tokens\n",
    "def cleaned_text(raw_text):\n",
    "    text_beg = raw_text.find(\"*** START OF\")\n",
    "    text_end = raw_text.find(\"*** END OF\")\n",
    "\n",
    "    raw_text = raw_text[text_beg : text_end]\n",
    "    raw_text = re.findall(r\"[a-zA-Z]+\", raw_text)\n",
    "    raw_text = \" \".join(raw_text).lower()\n",
    "    tokens = word_tokenize(raw_text)\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "#Getting cleaned tokens\n",
    "tokens = []\n",
    "for i in range(len(raw_dataset)):\n",
    "    tokens.append(cleaned_text(raw_dataset[i]))\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b383adfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#function to extract author name, book name and labels\n",
    "book_author_mapping = {}\n",
    "def extract_book_author(raw_text, index):\n",
    "    label = chr(index + 97)\n",
    "        \n",
    "    title_beg = raw_text.find(\"Title\") + 7\n",
    "    title_end = raw_text.find(\"\\r\\n\\r\\nAuthor\")\n",
    "    title = raw_text[title_beg : title_end]\n",
    "\n",
    "    author_beg = raw_text.find(\"Author\") + 8\n",
    "    c = 0\n",
    "    while raw_text[author_beg + c] != \"\\r\":\n",
    "        c += 1\n",
    "        author_end = author_beg + c\n",
    "    author = raw_text[author_beg : author_end]\n",
    "    \n",
    "    book_author_mapping[label] = [title, author]\n",
    "    \n",
    "    return label, title, author\n",
    "\n",
    "#getting the author name, book name, labels\n",
    "label, author, title = [], [], []\n",
    "for i in range(len(raw_dataset)):\n",
    "    label_temp, title_temp, author_temp = extract_book_author(raw_dataset[i],i)\n",
    "    label.append(label_temp)\n",
    "    author.append(author_temp)\n",
    "    title.append(title_temp)\n",
    "    \n",
    "print(len(label))\n",
    "print(len(author))\n",
    "print(len(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ada2e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to partition data\n",
    "def partitioned_text(tokens, title, author, label):\n",
    "    final_book_partitions = []\n",
    "    book_partitions = []\n",
    "    for j in range(len(tokens)):\n",
    "        subtractor = len(tokens[j])\n",
    "        for i in range(0, 200):\n",
    "            if(subtractor > 150):\n",
    "                partition = [\" \".join(tokens[j][i : i + 150]), title[j], author[j], label[j]]\n",
    "                book_partitions.append(partition)\n",
    "                subtractor -= 150\n",
    "    return book_partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdc0bfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partitions</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>astronomy ii astronomy seventeenth century iii...</td>\n",
       "      <td>The Astronomy of Milton's 'Paradise Lost'</td>\n",
       "      <td>Thomas Orchard</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>chapter letter sound vowel consonant exercise ...</td>\n",
       "      <td>The Art Of Writing &amp; Speaking The English Lang...</td>\n",
       "      <td>Sherwin Cody</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>rock stratification imbedded fossil volcanic r...</td>\n",
       "      <td>The Studentâ€™s Elements of Geology</td>\n",
       "      <td>Sir Charles Lyell</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>lost thomas n orchard member british astronomi...</td>\n",
       "      <td>The Astronomy of Milton's 'Paradise Lost'</td>\n",
       "      <td>Thomas Orchard</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>branch needlework whole work interspersed colo...</td>\n",
       "      <td>Beeton's Book of Needlework</td>\n",
       "      <td>Isabella Beeton</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>knitting netting berlin wool work point lace g...</td>\n",
       "      <td>Beeton's Book of Needlework</td>\n",
       "      <td>Isabella Beeton</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>punctuation hyphenation inconsistency correcte...</td>\n",
       "      <td>The Astronomy of Milton's 'Paradise Lost'</td>\n",
       "      <td>Thomas Orchard</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>promised dedication entirely different novel m...</td>\n",
       "      <td>Murder in the Gunroom</td>\n",
       "      <td>Henry Beam Piper</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>letter sound vowel consonant exercise dictiona...</td>\n",
       "      <td>The Art Of Writing &amp; Speaking The English Lang...</td>\n",
       "      <td>Sherwin Cody</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>prefixed page devoted separate branch needlewo...</td>\n",
       "      <td>Beeton's Book of Needlework</td>\n",
       "      <td>Isabella Beeton</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            partitions  \\\n",
       "356  astronomy ii astronomy seventeenth century iii...   \n",
       "126  chapter letter sound vowel consonant exercise ...   \n",
       "855  rock stratification imbedded fossil volcanic r...   \n",
       "318  lost thomas n orchard member british astronomi...   \n",
       "592  branch needlework whole work interspersed colo...   \n",
       "..                                                 ...   \n",
       "579  knitting netting berlin wool work point lace g...   \n",
       "255  punctuation hyphenation inconsistency correcte...   \n",
       "637  promised dedication entirely different novel m...   \n",
       "127  letter sound vowel consonant exercise dictiona...   \n",
       "588  prefixed page devoted separate branch needlewo...   \n",
       "\n",
       "                                                 title             author  \\\n",
       "356          The Astronomy of Milton's 'Paradise Lost'     Thomas Orchard   \n",
       "126  The Art Of Writing & Speaking The English Lang...       Sherwin Cody   \n",
       "855                  The Studentâ€™s Elements of Geology  Sir Charles Lyell   \n",
       "318          The Astronomy of Milton's 'Paradise Lost'     Thomas Orchard   \n",
       "592                        Beeton's Book of Needlework    Isabella Beeton   \n",
       "..                                                 ...                ...   \n",
       "579                        Beeton's Book of Needlework    Isabella Beeton   \n",
       "255          The Astronomy of Milton's 'Paradise Lost'     Thomas Orchard   \n",
       "637                              Murder in the Gunroom   Henry Beam Piper   \n",
       "127  The Art Of Writing & Speaking The English Lang...       Sherwin Cody   \n",
       "588                        Beeton's Book of Needlework    Isabella Beeton   \n",
       "\n",
       "    label  \n",
       "356     b  \n",
       "126     a  \n",
       "855     e  \n",
       "318     b  \n",
       "592     c  \n",
       "..    ...  \n",
       "579     c  \n",
       "255     b  \n",
       "637     d  \n",
       "127     a  \n",
       "588     c  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting partitoins\n",
    "partitions = pd.DataFrame(partitioned_text(tokens, title, author, label),columns = ['partitions', 'title', 'author', 'label'])\n",
    "\n",
    "partitions = shuffle(partitions)\n",
    "partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f7c076",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494105fe",
   "metadata": {},
   "source": [
    "## BOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "247077e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abdegilns</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absurd</th>\n",
       "      <th>accident</th>\n",
       "      <th>according</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>acid</th>\n",
       "      <th>acknowledged</th>\n",
       "      <th>action</th>\n",
       "      <th>...</th>\n",
       "      <th>write</th>\n",
       "      <th>writer</th>\n",
       "      <th>writing</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrought</th>\n",
       "      <th>www</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 1041 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     abdegilns  ability  able  absurd  accident  according  accuracy  acid  \\\n",
       "0            0        0     1       0         0          0         0     0   \n",
       "1            0        0     0       0         0          0         0     0   \n",
       "2            0        0     0       0         0          0         0     0   \n",
       "3            0        0     1       0         0          0         0     0   \n",
       "4            0        0     0       0         0          0         0     0   \n",
       "..         ...      ...   ...     ...       ...        ...       ...   ...   \n",
       "995          0        0     0       0         0          0         0     0   \n",
       "996          0        0     0       0         0          0         0     0   \n",
       "997          0        0     0       0         1          0         0     0   \n",
       "998          0        0     0       0         0          0         0     0   \n",
       "999          0        0     0       0         0          0         0     0   \n",
       "\n",
       "     acknowledged  action  ...  write  writer  writing  wrong  wrought  www  \\\n",
       "0               0       0  ...      0       2        0      0        0    0   \n",
       "1               1       0  ...      2       0        2      0        0    0   \n",
       "2               0       0  ...      0       0        0      0        0    0   \n",
       "3               0       0  ...      0       1        0      0        0    0   \n",
       "4               0       0  ...      0       0        0      0        0    0   \n",
       "..            ...     ...  ...    ...     ...      ...    ...      ...  ...   \n",
       "995             0       0  ...      0       0        0      0        0    0   \n",
       "996             0       0  ...      0       0        0      0        0    0   \n",
       "997             0       0  ...      0       0        0      0        0    0   \n",
       "998             1       0  ...      2       0        2      0        0    0   \n",
       "999             0       0  ...      0       0        0      0        0    0   \n",
       "\n",
       "     year  yet  york  young  \n",
       "0       0    0     0      1  \n",
       "1       0    1     0      0  \n",
       "2       0    0     0      0  \n",
       "3       0    0     1      0  \n",
       "4       1    0     0      0  \n",
       "..    ...  ...   ...    ...  \n",
       "995     1    0     0      0  \n",
       "996     0    0     1      0  \n",
       "997     2    0     0      0  \n",
       "998     0    1     0      0  \n",
       "999     1    0     0      0  \n",
       "\n",
       "[1000 rows x 1041 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_BOW(data): # take the partitions\n",
    "    cv = CountVectorizer()\n",
    "    cv_fit = cv.fit_transform(data.iloc[:,0]).toarray()\n",
    "    cv_fit_df = pd.DataFrame(cv_fit, columns = cv.get_feature_names())\n",
    "    return cv_fit_df\n",
    "    \n",
    "\n",
    "bow_vectorizer = model_BOW(partitions)\n",
    "bow_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076d3544",
   "metadata": {},
   "source": [
    "## TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9066190e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abdegilns</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absurd</th>\n",
       "      <th>accident</th>\n",
       "      <th>according</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>acid</th>\n",
       "      <th>acknowledged</th>\n",
       "      <th>action</th>\n",
       "      <th>...</th>\n",
       "      <th>write</th>\n",
       "      <th>writer</th>\n",
       "      <th>writing</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrought</th>\n",
       "      <th>www</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078777</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056344</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048139</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.104794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 1041 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     abdegilns  ability      able  absurd  accident  according  accuracy  \\\n",
       "0          0.0      0.0  0.076399     0.0  0.000000        0.0       0.0   \n",
       "1          0.0      0.0  0.000000     0.0  0.000000        0.0       0.0   \n",
       "2          0.0      0.0  0.000000     0.0  0.000000        0.0       0.0   \n",
       "3          0.0      0.0  0.080242     0.0  0.000000        0.0       0.0   \n",
       "4          0.0      0.0  0.000000     0.0  0.000000        0.0       0.0   \n",
       "..         ...      ...       ...     ...       ...        ...       ...   \n",
       "995        0.0      0.0  0.000000     0.0  0.000000        0.0       0.0   \n",
       "996        0.0      0.0  0.000000     0.0  0.000000        0.0       0.0   \n",
       "997        0.0      0.0  0.000000     0.0  0.085598        0.0       0.0   \n",
       "998        0.0      0.0  0.000000     0.0  0.000000        0.0       0.0   \n",
       "999        0.0      0.0  0.000000     0.0  0.000000        0.0       0.0   \n",
       "\n",
       "     acid  acknowledged  action  ...     write    writer   writing  wrong  \\\n",
       "0     0.0      0.000000     0.0  ...  0.000000  0.153672  0.000000    0.0   \n",
       "1     0.0      0.076724     0.0  ...  0.141259  0.000000  0.139438    0.0   \n",
       "2     0.0      0.000000     0.0  ...  0.000000  0.000000  0.000000    0.0   \n",
       "3     0.0      0.000000     0.0  ...  0.000000  0.080701  0.000000    0.0   \n",
       "4     0.0      0.000000     0.0  ...  0.000000  0.000000  0.000000    0.0   \n",
       "..    ...           ...     ...  ...       ...       ...       ...    ...   \n",
       "995   0.0      0.000000     0.0  ...  0.000000  0.000000  0.000000    0.0   \n",
       "996   0.0      0.000000     0.0  ...  0.000000  0.000000  0.000000    0.0   \n",
       "997   0.0      0.000000     0.0  ...  0.000000  0.000000  0.000000    0.0   \n",
       "998   0.0      0.076857     0.0  ...  0.141505  0.000000  0.139681    0.0   \n",
       "999   0.0      0.000000     0.0  ...  0.000000  0.000000  0.000000    0.0   \n",
       "\n",
       "     wrought  www      year       yet      york     young  \n",
       "0        0.0  0.0  0.000000  0.000000  0.000000  0.094241  \n",
       "1        0.0  0.0  0.000000  0.078777  0.000000  0.000000  \n",
       "2        0.0  0.0  0.000000  0.000000  0.000000  0.000000  \n",
       "3        0.0  0.0  0.000000  0.000000  0.056344  0.000000  \n",
       "4        0.0  0.0  0.039251  0.000000  0.000000  0.000000  \n",
       "..       ...  ...       ...       ...       ...       ...  \n",
       "995      0.0  0.0  0.041299  0.000000  0.000000  0.000000  \n",
       "996      0.0  0.0  0.000000  0.000000  0.048139  0.000000  \n",
       "997      0.0  0.0  0.104794  0.000000  0.000000  0.000000  \n",
       "998      0.0  0.0  0.000000  0.078914  0.000000  0.000000  \n",
       "999      0.0  0.0  0.040142  0.000000  0.000000  0.000000  \n",
       "\n",
       "[1000 rows x 1041 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_TFIDF(data): # take the partitions\n",
    "    tfIdf = TfidfVectorizer(use_idf=True)\n",
    "    tfidf_fit = tfIdf.fit_transform(data.iloc[:,0]).toarray()\n",
    "    tfidf_fit_df = pd.DataFrame(tfidf_fit, columns = tfIdf.get_feature_names())\n",
    "    return tfidf_fit_df\n",
    "\n",
    "tfidf_vectorizer = model_TFIDF(partitions)\n",
    "tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e153383",
   "metadata": {},
   "source": [
    "## LDA as a feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c48d632e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00135284, 0.00135267, 0.00135459, 0.00135999, 0.99457991],\n",
       "       [0.00136088, 0.00136165, 0.00135649, 0.00135641, 0.99456457],\n",
       "       [0.00132641, 0.57571221, 0.42030756, 0.00132671, 0.0013271 ],\n",
       "       ...,\n",
       "       [0.99469231, 0.00132588, 0.00132752, 0.00132781, 0.00132648],\n",
       "       [0.00136078, 0.0013613 , 0.00135625, 0.00135641, 0.99456525],\n",
       "       [0.00133649, 0.00133375, 0.0013366 , 0.99465246, 0.0013407 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_LDA2(data): # take the partitions\n",
    "    cv = CountVectorizer(max_df = 0.9, min_df = 2)\n",
    "    dtm = cv.fit_transform(data)\n",
    "    lda = LatentDirichletAllocation(n_components = 5, random_state = 5)\n",
    "    lda = lda.fit_transform(dtm)\n",
    "    return lda\n",
    "\n",
    "lda = model_LDA2(partitions['partitions'])\n",
    "lda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0ee0c",
   "metadata": {},
   "source": [
    "## wordembedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caae3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions_splitted = []\n",
    "for partition in partitions['partitions']:\n",
    "    temp = partition.split()\n",
    "    partitions_splitted.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af3b61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_word2vec(tokens_data): # take the partitions_splitted\n",
    "    model = Word2Vec(sentences = tokens_data, vector_size = 150, workers = 6)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73c1bc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1c086cef7f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model = model_word2vec(partitions_splitted)\n",
    "word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "478f70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the word document vectors\n",
    "def doc_vectors(data_tokens, word2vec_model):\n",
    "    features = []\n",
    "    for tokens in data_tokens:\n",
    "        zero_vector = np.zeros(word2vec_model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in word2vec_model.wv:\n",
    "                vectors.append(word2vec_model.wv[token])\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis = 0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features\n",
    "\n",
    "vectorized_doc = doc_vectors(partitions_splitted, word2vec_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c06d9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 150)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_doc), len(vectorized_doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92163eae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
